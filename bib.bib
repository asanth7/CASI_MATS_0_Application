@online{arnav2025cot,
  title = {{{CoT Red-Handed}}: {{Stress Testing Chain-of-Thought Monitoring}}},
  author = {Arnav, Benjamin and Bernabeu-Pérez, Pablo and Helm-Burger, Nathan and Kostolansky, Tim and Whittingham, Hannes and Phuong, Mary},
  date = {2025-06-29},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2505.23575},
  pubstate = {prepublished}
}

@inproceedings{korbak2025chain,
  title = {Chain of {{Thought Monitorability}}: {{A New}} and {{Fragile Opportunity}} for {{AI Safety}}},
  author = {Korbak, Tomek and Balesni, Mikita and Barnes, Elizabeth and Bengio, Yoshua and Benton, Joe and Bloom, Joseph and Chen, Mark and Cooney, Alan and Dafoe, Allan and Dragan, Anca and Emmons, Scott and Evans, Owain and Farhi, David and Greenblatt, Ryan and Hendrycks, Dan and Hobbhahn, Marius and Hubinger, Evan and Irving, Geoffrey and Jenner, Erik and Kokotajlo, Daniel and Krakovna, Victoria and Legg, Shane and Lindner, David and Luan, David and Mądry, Aleksander and Michael, Julian and Nanda, Neel and Orr, Dave and Pachocki, Jakub and Perez, Ethan and Phuong, Mary and Roger, Fabien and Saxe, Joshua and Shlegeris, Buck and Soto, Martín and Steinberger, Eric and Wang, Jasmine and Zaremba, Wojciech and Baker, Bowen and Shah, Rohin and Mikulik, Vlad},
  date = {2025-07-15},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2507.11473},
  pubstate = {prepublished}
}

@online{emmons2025whenb,
  title = {When {{Chain}} of {{Thought}} Is {{Necessary}}, {{Language Models Struggle}} to {{Evade Monitors}}},
  author = {Emmons, Scott and Jenner, Erik and Elson, David K. and Saurous, Rif A. and Rajamanoharan, Senthooran and Chen, Heng and Shafkat, Irhum and Shah, Rohin},
  date = {2025-07-07},
  url = {http://arxiv.org/abs/2507.05246},
  pubstate = {prepublished}
}

@article{korbak2025if,
  title = {If You Can Generate Obfuscated Chain-of-Thought, Can You Monitor It?},
  author = {Korbak, Tomek and Stickland, Asa Cooper},
  date = {2025-08-04},
  url = {https://www.lesswrong.com/posts/ZEdP6rYirxPxRSfTb/if-you-can-generate-obfuscated-chain-of-thought-can-you}
}

@online{greenblatt2024aicontrol,
  title = {{{AI Control}}: {{Improving Safety Despite Intentional Subversion}}},
  author = {Greenblatt, Ryan and Shlegeris, Buck and Sachan, Kshitij and Roger, Fabien},
  date = {2024-07-23},
  url = {http://arxiv.org/abs/2312.06942},
  pubstate = {prepublished}
}

@online{karvonen2025robustly,
  title = {Robustly {{Improving LLM Fairness}} in {{Realistic Settings}} via {{Interpretability}}},
  author = {Karvonen, Adam and Marks, Samuel},
  date = {2025-06-12},
  url = {http://arxiv.org/abs/2506.10922},
  pubstate = {prepublished}
}

@online{korbak2025chaina,
  title = {Chain of {{Thought Monitorability}}: {{A New}} and {{Fragile Opportunity}} for {{AI Safety}}},
  author = {Korbak, Tomek and Balesni, Mikita and Barnes, Elizabeth and Bengio, Yoshua and Benton, Joe and Bloom, Joseph and Chen, Mark and Cooney, Alan and Dafoe, Allan and Dragan, Anca and Emmons, Scott and Evans, Owain and Farhi, David and Greenblatt, Ryan and Hendrycks, Dan and Hobbhahn, Marius and Hubinger, Evan and Irving, Geoffrey and Jenner, Erik and Kokotajlo, Daniel and Krakovna, Victoria and Legg, Shane and Lindner, David and Luan, David and Mądry, Aleksander and Michael, Julian and Nanda, Neel and Orr, Dave and Pachocki, Jakub and Perez, Ethan and Phuong, Mary and Roger, Fabien and Saxe, Joshua and Shlegeris, Buck and Soto, Martín and Steinberger, Eric and Wang, Jasmine and Zaremba, Wojciech and Baker, Bowen and Shah, Rohin and Mikulik, Vlad},
  date = {2025-07-15},
  url = {http://arxiv.org/abs/2507.11473},
  pubstate = {prepublished}
}

@online{roger2025reasoning,
  title = {Do Reasoning Models Use Their Scratchpad like We Do? {{Evidence}} from Distilling Paraphrases},
  author = {Roger, Fabian},
  date={2025-03-11},
  url = {https://alignment.anthropic.com/2025/distill-paraphrases/},
  file = {/Users/matanshtepel/Zotero/storage/4HXEAHNX/distill-paraphrases.html}
}

@article{goldowsky-dill2025claude,
  title = {Claude {{Sonnet}} 3.7 (Often) Knows When It’s in Alignment Evaluations},
  author = {Goldowsky-Dill, Nicholas and Balesni, Mikita and Scheurer, Jérémy and Hobbhahn, Marius},
  date = {2025-03-17},
  url = {https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment},
  langid = {english}
}

@online{mathew2024hidden,
  title = {Hidden in {{Plain Text}}: {{Emergence}} \& {{Mitigation}} of {{Steganographic Collusion}} in {{LLMs}}},
  author = {Mathew, Yohan and Matthews, Ollie and McCarthy, Robert and Velja, Joan and family=Witt, given=Christian Schroeder, prefix=de, useprefix=false and Cope, Dylan and Schoots, Nandi},
  date = {2024-10-02},
  url = {http://arxiv.org/abs/2410.03768},
  pubstate = {prepublished}
}

@online{chen2025reasoning,
  title = {Reasoning {{Models Don}}'t {{Always Say What They Think}}},
  author = {Chen, Yanda and Benton, Joe and Radhakrishnan, Ansh and Uesato, Jonathan and Denison, Carson and Schulman, John and Somani, Arushi and Hase, Peter and Wagner, Misha and Roger, Fabien and Mikulik, Vlad and Bowman, Samuel R. and Leike, Jan and Kaplan, Jared and Perez, Ethan},
  date = {2025-05-08},
  url = {http://arxiv.org/abs/2505.05410},
  pubstate = {prepublished}
}

@online{baker2025monitoringa,
  title = {Monitoring {{Reasoning Models}} for {{Misbehavior}} and the {{Risks}} of {{Promoting Obfuscation}}},
  author = {Baker, Bowen and Huizinga, Joost and Gao, Leo and Dou, Zehao and Guan, Melody Y. and Madry, Aleksander and Zaremba, Wojciech and Pachocki, Jakub and Farhi, David},
  date = {2025-03-14},
  url = {http://arxiv.org/abs/2503.11926},
  pubstate = {prepublished}
}

@article{arike2025extractandevaluate,
  title = {Extract-and-{{Evaluate Monitoring Can Significantly Enhance CoT Monitor Performance}} ({{Research Note}})},
  author = {Arike, Rauno and RohanS and Biswas, Shubhorup and RohanS and Biswas, Shubhorup},
  date = {2025-08-08},
  url = {https://www.lesswrong.com/posts/nRcKDYi2KfRTXdvDF/extract-and-evaluate-monitoring-can-significantly-enhance}
}

@online{kutasov2025shadearena,
  title = {{{SHADE-Arena}}: {{Evaluating Sabotage}} and {{Monitoring}} in {{LLM Agents}}},
  author = {Kutasov, Jonathan and Sun, Yuqi and Colognese, Paul and family=Weij, given=Teun, prefix=van der, useprefix=false and Petrini, Linda and Zhang, Chen Bo Calvin and Hughes, John and Deng, Xiang and Sleight, Henry and Tracy, Tyler and Shlegeris, Buck and Benton, Joe},
  date = {2025-07-08},
  url = {http://arxiv.org/abs/2506.15740},
  pubstate = {prepublished}
}

@article{macar2025unfaithful,
  title = {Unfaithful Chain-of-Thought as Nudged Reasoning},
  author = {Macar, Uzay and Conmy, Arthur and Nanda, Neel and Bogdan, Paul and Macar, Uzay and Conmy, Arthur and Nanda, Neel},
  date = {2025-07-22},
  url = {https://www.lesswrong.com/posts/vPAFPpRDEg3vjhNFi/unfaithful-chain-of-thought-as-nudged-reasoning}
}

@online{chua2025are,
  title = {Are {{DeepSeek R1 And Other Reasoning Models More Faithful}}?},
  author = {Chua, James and Evans, Owain},
  date = {2025-07-15},
  url = {http://arxiv.org/abs/2501.08156},
  pubstate = {prepublished}
}

@online{atanasova2023faithfulness,
  title = {Faithfulness {{Tests}} for {{Natural Language Explanations}}},
  author = {Atanasova, Pepa and Camburu, Oana-Maria and Lioma, Christina and Lukasiewicz, Thomas and Simonsen, Jakob Grue and Augenstein, Isabelle},
  date = {2023-06-30},
  url = {http://arxiv.org/abs/2305.18029},
  pubstate = {prepublished}
}


@article{richbc2025building,
  title = {Building {{Black-box Scheming Monitors}}},
  author = {{richbc} and Storf, Simon and Hobbhahn, Marius and {james\_\_p} and {richbc} and Storf, Simon and Hobbhahn, Marius},
  date = {2025-07-29},
  url = {https://www.lesswrong.com/posts/sb8WmKNgwzefa6oaJ/building-black-box-scheming-monitors}
}

@online{chennabasappa2025llamafirewall,
  title = {{{LlamaFirewall}}: {{An}} Open Source Guardrail System for Building Secure {{AI}} Agents},
  author = {Chennabasappa, Sahana and Nikolaidis, Cyrus and Song, Daniel and Molnar, David and Ding, Stephanie and Wan, Shengye and Whitman, Spencer and Deason, Lauren and Doucette, Nicholas and Montilla, Abraham and Gampa, Alekhya and family=Paola, given=Beto, prefix=de, useprefix=false and Gabi, Dominik and Crnkovich, James and Testud, Jean-Christophe and He, Kat and Chaturvedi, Rashnil and Zhou, Wu and Saxe, Joshua},
  date = {2025-05-06},
  url = {http://arxiv.org/abs/2505.03574},
  pubstate = {prepublished}
}

@online{binder2024lookinga,
  title = {Looking {{Inward}}: {{Language Models Can Learn About Themselves}} by {{Introspection}}},
  author = {Binder, Felix J. and Chua, James and Korbak, Tomek and Sleight, Henry and Hughes, John and Long, Robert and Perez, Ethan and Turpin, Miles and Evans, Owain},
  date = {2024-10-17},
  url = {http://arxiv.org/abs/2410.13787},
  pubstate = {prepublished}
}

@online{arcuschin2025chainofthoughta,
  title = {Chain-of-{{Thought Reasoning In The Wild Is Not Always Faithful}}},
  author = {Arcuschin, Iván and Janiak, Jett and Krzyzanowski, Robert and Rajamanoharan, Senthooran and Nanda, Neel and Conmy, Arthur},
  date = {2025-06-17},
  url = {http://arxiv.org/abs/2503.08679},
  pubstate = {prepublished}
}

@online{tanneru2024hardness,
  title = {On the {{Hardness}} of {{Faithful Chain-of-Thought Reasoning}} in {{Large Language Models}}},
  author = {Tanneru, Sree Harsha and Ley, Dan and Agarwal, Chirag and Lakkaraju, Himabindu},
  date = {2024-07-01},
  url = {http://arxiv.org/abs/2406.10625},
  pubstate = {prepublished}
}

@online{turpin2023languagea,
  title = {Language {{Models Don}}'t {{Always Say What They Think}}: {{Unfaithful Explanations}} in {{Chain-of-Thought Prompting}}},
  author = {Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R.},
  date = {2023-12-09},
  url = {http://arxiv.org/abs/2305.04388},
  pubstate = {prepublished}
}

@online{lanham2023measuringb,
  title = {Measuring {{Faithfulness}} in {{Chain-of-Thought Reasoning}}},
  author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Lukošiūtė, Kamilė and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and Telleen-Lawton, Timothy and Hume, Tristan and Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
  date = {2023-07-17},
  url = {http://arxiv.org/abs/2307.13702},
  pubstate = {prepublished}
}

@online{tutek2025measuringa,
  title = {Measuring {{Chain}} of {{Thought Faithfulness}} by {{Unlearning Reasoning Steps}}},
  author = {Tutek, Martin and Chaleshtori, Fateme Hashemi and Marasović, Ana and Belinkov, Yonatan},
  date = {2025-06-12},
  url = {http://arxiv.org/abs/2502.14829},
  pubstate = {prepublished}
}

%%%%%%%%%%%%%%%%%%%%%%%% Alex-casi misuse

@online{jones2024adversariesb,
  title = {Adversaries {{Can Misuse Combinations}} of {{Safe Models}}},
  author = {Jones, Erik and Dragan, Anca and Steinhardt, Jacob},
  date = {2024-07-01},
  url = {http://arxiv.org/abs/2406.14595},
  pubstate = {prepublished}
}

@online{chao2024jailbreakinga,
  title = {Jailbreaking {{Black Box Large Language Models}} in {{Twenty Queries}}},
  author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
  date = {2024-07-18},
  url = {http://arxiv.org/abs/2310.08419},
  pubstate = {prepublished}
}

@online{glukhov2024breach,
  title = {Breach {{By A Thousand Leaks}}: {{Unsafe Information Leakage}} in `{{Safe}}' {{AI Responses}}},
  author = {Glukhov, David and Han, Ziwen and Shumailov, Ilia and Papyan, Vardan and Papernot, Nicolas},
  date = {2024-10-30},
  url = {http://arxiv.org/abs/2407.02551},
  pubstate = {prepublished}
}

@online{brown2025benchmarking,
  title = {Benchmarking {{Misuse Mitigation Against Covert Adversaries}}},
  author = {Brown, Davis and Sabbaghi, Mahdi and Sun, Luze and Robey, Alexander and Pappas, George J. and Wong, Eric and Hassani, Hamed},
  date = {2025-06-06},
  url = {http://arxiv.org/abs/2506.06414},
  pubstate = {prepublished}
}

@online{gotting2025virology,
  title = {Virology {{Capabilities Test}} ({{VCT}}): {{A Multimodal Virology Q}}\&{{A Benchmark}}},
  author = {Götting, Jasper and Medeiros, Pedro and Sanders, Jon G. and Li, Nathaniel and Phan, Long and Elabd, Karam and Justen, Lennart and Hendrycks, Dan and Donoughe, Seth},
  date = {2025-04-21},
  url = {http://arxiv.org/abs/2504.16137},
  pubstate = {prepublished},
  version = {1}
}

@online{zou2023universal,
  title = {Universal and {{Transferable Adversarial Attacks}} on {{Aligned Language Models}}},
  author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
  date = {2023-12-20},
  url = {http://arxiv.org/abs/2307.15043},
  pubstate = {prepublished}
}
  
@misc{li_drattack_2024,
    title = {{DrAttack}: {Prompt} {Decomposition} and {Reconstruction} {Makes} {Powerful} {LLM} {Jailbreakers}},
    shorttitle = {{DrAttack}},
    url = {http://arxiv.org/abs/2402.16914},
    doi = {10.48550/arXiv.2402.16914},
    abstract = {The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt {\textbackslash}textbf\{D\}ecomposition and {\textbackslash}textbf\{R\}econstruction framework for jailbreak {\textbackslash}textbf\{Attack\} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts' synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0{\textbackslash}\% on GPT-4 with merely 15 queries surpassed previous art by 33.1{\textbackslash}\%. The project is available at https://github.com/xirui-li/DrAttack.},
    urldate = {2025-11-03},
    publisher = {arXiv},
    author = {Li, Xirui and Wang, Ruochen and Cheng, Minhao and Zhou, Tianyi and Hsieh, Cho-Jui},
    month = nov,
    year = {2024},
    note = {arXiv:2402.16914 [cs]
version: 3},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}