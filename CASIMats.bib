
@misc{li_drattack_2024,
	title = {{DrAttack}: Prompt Decomposition and Reconstruction Makes Powerful {LLM} Jailbreakers},
	url = {http://arxiv.org/abs/2402.16914},
	doi = {10.48550/arXiv.2402.16914},
	shorttitle = {{DrAttack}},
	abstract = {The safety alignment of Large Language Models ({LLMs}) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger {LLMs} to output harmful content. However, current methods for jailbreaking {LLMs}, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned {LLMs}. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt {\textbackslash}textbf\{D\}ecomposition and {\textbackslash}textbf\{R\}econstruction framework for jailbreak {\textbackslash}textbf\{Attack\} ({DrAttack}). {DrAttack} includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts' synonyms that maintain the original intent while jailbreaking {LLMs}. An extensive empirical study across multiple open-source and closed-source {LLMs} demonstrates that, with a significantly reduced number of queries, {DrAttack} obtains a substantial gain of success rate over prior {SOTA} prompt-only attackers. Notably, the success rate of 78.0{\textbackslash}\% on {GPT}-4 with merely 15 queries surpassed previous art by 33.1{\textbackslash}\%. The project is available at https://github.com/xirui-li/{DrAttack}.},
	number = {{arXiv}:2402.16914},
	publisher = {{arXiv}},
	author = {Li, Xirui and Wang, Ruochen and Cheng, Minhao and Zhou, Tianyi and Hsieh, Cho-Jui},
	urldate = {2025-11-03},
	date = {2024-11-11},
	eprinttype = {arxiv},
	eprint = {2402.16914 [cs]},
	note = {version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Full Text PDF:C\:\\Users\\aravs\\Zotero\\storage\\WXUY7M52\\Li et al. - 2024 - DrAttack Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers.pdf:application/pdf;Snapshot:C\:\\Users\\aravs\\Zotero\\storage\\NAB3IWBA\\2402.html:text/html},
}

@misc{yueh-han_monitoring_2025,
	title = {Monitoring Decomposition Attacks in {LLMs} with Lightweight Sequential Monitors},
	url = {http://arxiv.org/abs/2506.10949},
	doi = {10.48550/arXiv.2506.10949},
	abstract = {Current {LLM} safety defenses fail under decomposition attacks, where a malicious goal is decomposed into benign subtasks that circumvent refusals. The challenge lies in the existing shallow safety alignment techniques: they only detect harm in the immediate prompt and do not reason about long-range intent, leaving them blind to malicious intent that emerges over a sequence of seemingly benign instructions. We therefore propose adding an external monitor that observes the conversation at a higher granularity. To facilitate our study of monitoring decomposition attacks, we curate the largest and most diverse dataset to date, including question-answering, text-to-image, and agentic tasks. We verify our datasets by testing them on frontier {LLMs} and show an 87\% attack success rate on average on {GPT}-4o. This confirms that decomposition attack is broadly effective. Additionally, we find that random tasks can be injected into the decomposed subtasks to further obfuscate malicious intents. To defend in real time, we propose a lightweight sequential monitoring framework that cumulatively evaluates each subtask. We show that a carefully prompt engineered lightweight monitor achieves a 93\% defense success rate, beating reasoning models like o3 mini as a monitor. Moreover, it remains robust against random task injection and cuts cost by 90\% and latency by 50\%. Our findings suggest that lightweight sequential monitors are highly effective in mitigating decomposition attacks and are viable in deployment.},
	number = {{arXiv}:2506.10949},
	publisher = {{arXiv}},
	author = {Yueh-Han, Chen and Joshi, Nitish and Chen, Yulin and Andriushchenko, Maksym and Angell, Rico and He, He},
	urldate = {2025-11-03},
	date = {2025-06-14},
	eprinttype = {arxiv},
	eprint = {2506.10949 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Full Text PDF:C\:\\Users\\aravs\\Zotero\\storage\\EH5SSL7P\\Yueh-Han et al. - 2025 - Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors.pdf:application/pdf;Snapshot:C\:\\Users\\aravs\\Zotero\\storage\\2W4LPFL8\\2506.html:text/html},
}

@inproceedings{srivastav_safe_2025,
	location = {Vienna, Austria},
	title = {Safe in Isolation, Dangerous Together: Agent-Driven Multi-Turn Decomposition Jailbreaks on {LLMs}},
	isbn = {979-8-89176-264-0},
	url = {https://aclanthology.org/2025.realm-1.13/},
	doi = {10.18653/v1/2025.realm-1.13},
	shorttitle = {Safe in Isolation, Dangerous Together},
	abstract = {Large Language Models ({LLMs}) are increasingly deployed in critical domains, but their vulnerability to jailbreak attacks remains a significant concern. In this paper, we propose a multi-agent, multi-turn jailbreak strategy that systematically bypasses {LLM} safety mechanisms by decomposing harmful queries into seemingly benign sub-tasks. Built upon a role-based agentic framework consisting of a Question Decomposer, a Sub-Question Answerer, and an Answer Combiner, we demonstrate how {LLMs} can be manipulated to generate prohibited content without prompt manipulations. Our results show a drastic increase in attack success, often exceeding 90\% across various {LLMs}, including {GPT}-3.5-Turbo, Gemma-2-9B, and Mistral-7B. We further analyze attack consistency across multiple runs and vulnerability across content categories. Compared to existing widely used jailbreak techniques, our multi-agent method consistently achieves the highest attack success rate across all evaluated models. These findings reveal a critical flaw in the current safety architecture of multi-agent {LLM} systems: their lack of holistic context awareness. By revealing this weakness, we argue for an urgent need to develop multi-turn, context-aware, and robust defenses to address this emerging threat vector.},
	pages = {170--183},
	booktitle = {Proceedings of the 1st Workshop for Research on Agent Language Models ({REALM} 2025)},
	publisher = {Association for Computational Linguistics},
	author = {Srivastav, Devansh and Zhang, Xiao},
	editor = {Kamalloo, Ehsan and Gontier, Nicolas and Lu, Xing Han and Dziri, Nouha and Murty, Shikhar and Lacoste, Alexandre},
	urldate = {2025-11-03},
	date = {2025-07},
	file = {Full Text PDF:C\:\\Users\\aravs\\Zotero\\storage\\XYE4JS9C\\Srivastav and Zhang - 2025 - Safe in Isolation, Dangerous Together Agent-Driven Multi-Turn Decomposition Jailbreaks on LLMs.pdf:application/pdf},
}

@article{zhang_exploiting_nodate,
	title = {Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for {LLMs}},
	abstract = {Recent advancements in large language models ({LLMs}) have notably improved their proficiency in executing complex tasks. However, these advancements are accompanied by an increased risk of generating toxic content as well as leaking private information. “Jailbreak” is an emerging trend to amplify this vulnerability by carefully modifying prompts such as “{DAN}” to circumvent the {LLMs}’ defense. Notwithstanding, existing jailbreaks typically focus on specific prompts or tokens, rendering them susceptible to countermeasures such as realignments. In contrast to these prompt-level or tokenlevel jailbreaks, we present a novel task-level jailbreak based on “knowledge decomposition” , which does not rely on any specific prompts or tokens. Our attack demonstrates significantly enhanced resistance against realignments compared to previous jailbreak techniques. Furthermore, our attack not only achieves about 10\% higher success rates than {SOTA} attacks but also generates responses that are richer in detail and information. This is attributed to aggregation of responses from multiple well-designed queries rather than relying on only a singular query as in previous attacks, thus signifying an elevated risk of threat. On the other hand, “knowledge decomposition” provide us a method to generate plenty tasks with varying risk levels, thereby establishing a novel benchmark to assess the defensive effectiveness of {LLMs}.},
	author = {Zhang, Lan and Gao, Xinben and Song, Jinke},
	langid = {english},
	file = {PDF:C\:\\Users\\aravs\\Zotero\\storage\\E96TBEMU\\Zhang et al. - Exploiting Task-Level Vulnerabilities An Automatic Jailbreak Attack and Defense Benchmarking for LL.pdf:application/pdf},
}

@misc{sun_multi-turn_2024,
	title = {Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles},
	url = {http://arxiv.org/abs/2408.04686},
	doi = {10.48550/arXiv.2408.04686},
	abstract = {Large language models ({LLMs}) have significantly enhanced the performance of numerous applications, from intelligent conversations to text generation. However, their inherent security vulnerabilities have become an increasingly significant challenge, especially with respect to jailbreak attacks. Attackers can circumvent the security mechanisms of these {LLMs}, breaching security constraints and causing harmful outputs. Focusing on multi-turn semantic jailbreak attacks, we observe that existing methods lack specific considerations for the role of multiturn dialogues in attack strategies, leading to semantic deviations during continuous interactions. Therefore, in this paper, we establish a theoretical foundation for multi-turn attacks by considering their support in jailbreak attacks, and based on this, propose a context-based contextual fusion black-box jailbreak attack method, named Context Fusion Attack ({CFA}). This method approach involves filtering and extracting key terms from the target, constructing contextual scenarios around these terms, dynamically integrating the target into the scenarios, replacing malicious key terms within the target, and thereby concealing the direct malicious intent. Through comparisons on various mainstream {LLMs} and red team datasets, we have demonstrated {CFA}'s superior success rate, divergence, and harmfulness compared to other multi-turn attack strategies, particularly showcasing significant advantages on Llama3 and {GPT}-4.},
	number = {{arXiv}:2408.04686},
	publisher = {{arXiv}},
	author = {Sun, Xiongtao and Zhang, Deyue and Yang, Dongdong and Zou, Quanchen and Li, Hui},
	urldate = {2025-11-03},
	date = {2024-08-08},
	eprinttype = {arxiv},
	eprint = {2408.04686 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\aravs\\Zotero\\storage\\RVZNVIDE\\Sun et al. - 2024 - Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles.pdf:application/pdf;Snapshot:C\:\\Users\\aravs\\Zotero\\storage\\DBW73EFC\\2408.html:text/html},
}
